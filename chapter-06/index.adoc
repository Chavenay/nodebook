:chapterNumber: 6
:chapterId: chapter-06
:sourceDir: ./examples
:sourceSample: TODO.js
:nodeCurrentVersion: v8
:v: 8
:nodeNextVersion: v9
:npmCurrentVersion: v5
:sectnums:
:revdate: {docdate}
:imagesdir: {indir}
:cross: &#x2718;
:tick: &#x2714;
:maybe: &#x2248;
ifdef::env[]
:imagesdir: .
endif::[]

= Déployer notre code

====
.Sommaire
- Déployer une application Node
- Choisir son hébergement
- Améliorer la portabilité
- Démarrer automatiquement nos applications
- Points de vigilance après une mise en ligne
====

[abstract]
--
--

include::../resources/tip-versions.adoc[]
include::../resources/tip-examples.adoc[]


[[deploy]]
== Déployer une application Node

Le choix de la technique de déploiement dépend de plusieurs facteurs qui se
renvoient à eux-même : l'hébergement peut dépendre du déploiement et vice-versa.

Je vous propose de partir balayer les différentes techniques de déploiement
avec des exemples et de voir quelles seraient les raisons d'opter pour l'une
d'entre elles.

Le choix est subjectif et vous appartient, en fonction de l'aisance que vous avez
à vous en emparer.
C'est un sujet qui prend du temps avant d'être maitrisé donc n'hésitez pas
à vous y reprendre à plusieurs fois.

[format="csv", options="header", separator=";"]
.Quelles techniques de déploiement s'utilisent avec quel type d'hébergement ?
|===
; <<hosting.paas,_PaaS_>>; <<hosting.shared,Mutualisé>>; <<hosting.cloud,Cloud>>; <<hosting.lambda,Lambda>>
<<deploy.notebook,Notebook Web>>; {cross}; {cross}; {cross}; {cross}
<<deploy.sftp,SSH/SFTP>>; {cross}; {tick}; {maybe}; {cross}
<<deploy.github,Import GitHub>>; {tick}; {cross}; {cross}; {cross}
<<deploy.cli,CLI>>; {tick}; {cross}; {tick}; {tick}
<<deploy.git,`git push`>>; {tick}; {cross}; {tick}; {cross}
<<deploy.clone,SSH + `git pull`>>; {tick}; {tick}; {tick}; {cross}
<<deploy.recipe,Recette>>; {tick}; {tick}; {tick}; {tick}
<<deploy.docker,`docker push`>>; {tick}; {cross}; {tick}; {cross}
<<deploy.ci,Intégration continue>>; {tick}; {tick}; {tick}; {tick}
|===

[[deploy.notebook]]
=== En codant dans un navigateur web

Le moyen le plus rapide d'exécuter du code Node sans avoir à se préoccuper
du déploiement est d'utiliser un service en ligne et de modifier du code
avec un navigateur web.

Je recommande _RunKit_ ([URL]#https://runkit.com/#) pour créer rapidement
du code qui tient dans un seul fichier, sans installer Node sur sa machine.
Le code est exécuté sur les serveurs de RunKit, le résultat s'affiche chez nous.
Les <<../chapter-05/index.adoc#modules,modules npm>>
(<<../chapter-05/index.adoc#,chapitre 5>>) sont installés automatiquement dans
leur version la plus récente.

.Exemple de _notebook_ RunKit dans le navigateur Firefox.
image::images/runkit-notebook.png[width="85%"]

RunKit propose aussi un modèle de <<lambda,fonction ephémère>> dont le
résultat devient accessible depuis une URL dédiée.
Essayez de copier/coller le code suivant dans un nouveau notebook en vous
rendant sur [URL]#https://runkit.com/new# :

[source,javascript]
.runkit-endpoint.js
----
include::{sourceDir}/runkit-endpoint.js[]
----
<1> Le module npm [URL]#https://npmjs.com/pokemon-random-name# exporte une fonction qui retourne un nom aléatoire de Pokémon.
<2> `exports.endpoint` est spécifique à RunKit et accepte une fonction identique à l'événement `server.on('request')` du <<../chapter-04/index.adoc#http,module `http`>> (<<../chapter-04/index.adoc#,chapitre 4>>).

Une fois sauvegardé et après avoir cliqué sur le lien *endpoint*,
un nouvel onglet s'ouvre et affiche un nom aléatoire de Pokémon.
C'est la valeur de retour passé à la réponse, comme on l'aurait fait
avec le <<../chapter-04/index.adoc#http,module `http`>>
(<<../chapter-04/index.adoc#,chapitre 4>>) ou dans une
<<../chapter-07/index.adoc#,application web>> (<<../chapter-07/index.adoc#,chapitre 7>>).

Le service en ligne _glitch_ ([URL]#https://glitch.com/#) permet d'aller
plus loin en développant, hébergeant et partageant des applications complètes.
Le service redéploie notre application à chaque changement.
Le fichier `.env` stocke les
<<../chapter-04/index.adoc#process.env,variables d'environnement>> de manière
sécurisée — nous seul y avons accès.

.Exemple d'application Node sur glitch.com.
image::images/glitch-app.png[width="85%"]

[TIP]
.[RemarquePreTitre]#Pratique# Console web
====
Glitch nous offre même une console web : un <<../chapter-04/index.adoc,terminal>>
entièrement fonctionnel, depuis un navigateur web !

Parfait pour <<../chapter-08/index.adoc#,coder un outil en ligne de commande>>
(<<../chapter-08/index.adoc#,chapitre 8>>) en travaillant depuis plusieurs
ordinateurs sans avoir à tout réinstaller à chaque fois.
====

[TIP]
.[RemarquePreTitre]#Pratique# Remixez les exemples de cet ouvrage
====
Vous pouvez créer votre premier projet sur glitch.
_Remixez_ cet ouvrage en cliquant en vous rendant sur
[URL]#+https://glitch.com/edit/#!/remix/nodebook+#.

Le contenu et les exemples seront copiés dans un nouveau projet,
exécutable et modifiable selon vos envies.
====


[[deploy.sftp]]
=== En transférant des fichiers via SSH

Transférer des fichiers est idéal pour débuter et lorsqu'on n'utilise pas Git
pour versionner son code.

Les services d'hébergement mutualisé, virtualisé ou dédié accordent
un accès à votre espace en ligne par le biais du protocole
SSH ([URL]#https://fr.wikipedia.org/wiki/Secure_Shell#).
Ce protocole crée une connexion sécurisée : les commandes saisies dans votre
terminal font effet sur la machine sur laquelle vous êtes connecté·e.

Des logiciels comme _FileZilla Client_ ([URL]#https://filezilla-project.org/#)
servent d'interfaces graphiques pour transférer des fichiers de notre machine
vers une machine distante. +
Les codes d'accès SSH se trouvent en général dans la section _Aide_ ou _Guides_
de votre hébergeur.

.Exemple de connexion à un serveur SSH distant avec FileZilla Client sous macOS.
image::images/filezilla-file-transfer.png[width="85%"]

[TIP]
.[RemarquePreTitre]#Windows# WinSCP
====
_WinSCP_ ([URL]#https://winscp.net#) est une alternative libre à
FileZilla pour Windows.
====

[NOTE]
.[RemarquePreTitre]#Avancé# `scp` et `rsync`
====
Notre terminal peut aussi servir à transféfer des fichiers.
Deux programmes se basent sur SSH et sont installés par défaut sur la plupart
des ordinateurs Linux et macOS :

- `scp` pour envoyer des fichiers de machine à machine
- `rsync` pour n'envoyer que les fichiers qui ont été modifiés ou supprimés
====

[[deploy.github]]
=== En important du code depuis GitHub

Importer du code depuis GitHub est la manière la plus simple de transférer
tous les fichiers versionnés sans être familier avec Git.

La plate-forme de <<deploy.notebook,programmation en ligne>> glitch
offre une option pour importer n'importe quel projet GitHub — à partir du moment
où le dépôt est public.

.Bouton d'import d'un dépôt GitHub sur glitch.com.
image::images/glitch-github-import.png[width="85%"]

Un clic sur le bouton btn:[Import from GitHub] ouvre une invite de saisie
destinée à mentionner le nom du dépôt GitHub à importer.
Le projet en cours sera entièrement remplacé par le contenu du dépôt distant.
C'est pratique pour récupérer des exercices ou ou apprendre en travaillant
sur du code écrit par quelqu'un·e d'autre.

[TIP]
.[RemarquePreTitre]#Pratique# Importer les exemples de cet ouvrage
====
Récupérez tout le contenu et les exemples de cet ouvrage
en recopiant `oncletom/nodebook` dans l'invite de saisie.
====

La <<paas,plate-forme de services>> Heroku ([URL]#https://heroku.com#)
pousse l'import GitHub un peu plus loin.
Sa fonctionnalité _déploie_ l'application à chaque nouveau commit.
L'application redémarre ensuite automatiquement pour prendre les changements en compte.

.Paramétrage de déploiement automatisé depuis un dépôt GitHub sur heroku.com.
image::images/heroku-github-import.png[width="85%"]

Une option nous permet de déployer une nouvelle version de l'application
à la suite d'une <<deploy.ci,intégration continue réussie>>.
Nous réduisons ainsi les risques de déployer une version défectueuse.


[[deploy.cli]]
=== Avec l'outil en ligne de commande de l'hébergeur

L'outil en ligne de commande d'un hébergeur permet de gérer les déploiements
_et_ d'autres aspects de l'hébergement en même temps.

La <<paas,plate-forme de services>> _now_ ([URL]#https://zeit.co/now#)
est un exemple de simplicité à ce niveau.

.Installation et configuration de l'outil `now`
----
$ npm install -g now
$ now login
----

Dans un terminal, déplacez-vous vers le répertoire de l'application à déployer.
Il suffit de taper `now` pour transférer les fichiers.
Les dépendances s'installent et le déploiement est accessible quelques secondes
plus tard :

----
$ now
Deploying ~/workspace/dtc-innovation/food-coops-dashboards
> Using Node.js 9.10.1 (requested: `>=8.0.0`)
> https://food-coops-dashboards-okgwzegyus.now.sh
> Synced 1 file (169.84KB) [11s]
> Building...
> ▲ npm install
> ✓ Using "package-lock.json"
> ⧗ Installing 9 main dependencies...
> ▲ npm install
> added 389 packages in 8.609s
> ▲ Snapshotting deployment
> Build completed
> Verifying instantiation in bru1
> ✔ Scaled 1 instance in bru1 [31s]
> Success! Deployment ready
----

En optant pour l'offre payante, nous pouvons aussi gérer les noms de domaine et
sous-domaines en leur attribuant l'URL du déploiement :

----
$ now alias food-coops-dashboards-okgwzegyus.now.sh my-domain.com
----

[NOTE]
.[RemarquePreTitre]#Pratique# Application de bureau
====
Le client en ligne de commande existe en version graphique.
Un glissé/déposé de fichiers suffit à lancer un déploiement.

Il se télécharge sur [URL]#https://zeit.co/download#.
====

////
TODO bloqué par https://github.com/Gandi/gandi.cli/issues/255

L'hébergeur indépendant et français _Gandi_ ([URL]#https://gandi.net#)
dispose d'une offre plus complète avec son outil en ligne de commande
écrit en Python et documenté sur [URL]#https://cli.gandi.net/#.

Sa <<paas,plate-forme de service>> crée et déploie un dépôt Git en
quelques commandes :

----
$ gandi paas create --name mon-application --type nodejs --size S
$ gandi paas attach mon-application
$ git push gandi master
$ gandi deploy
----
////


L'outil de la <<paas,plate-forme de services>> Heroku suit une approche
légèrement différente.
Il nous informe de l'état de nos application et en augmente ou diminue
la quantité de ressources allouée à leur fonctionnement.
Il simplifie la configuration de Git et
<<deploy.git,délègue le déploiement>> à ce dernier.
L'outil se télécharge sur
[URL]#https://devcenter.heroku.com/articles/heroku-cli#.

.Configuration de l'outil `heroku`
----
$ heroku login
----

La commande `heroku apps:create` crée une nouvelle application chez Heroku.
On peut faire la même chose dans un navigateur web en nous rendant sur
[URL]#https://dashboard.heroku.com/new-app#.
La commande `heroku git:remote` associe notre copie locale Git à cette application :

.Configuration de notre dépôt Git pour en faire une application Heroku.
----
$ heroku apps:create --region eu mon-application
$ heroku git:remote --app mon-application
----

Il ne nous reste plus qu'à <<deploy.git,pousser notre code avec Git>>
pour terminer la mise en ligne.

[[deploy.git]]
=== En faisant `git push` depuis sa machine

Le déploiement d'une branche Git est le moyen le plus facile d'automatiser
tous les aspects d'un déploiement.

Cette méthode est privilégiée par les <<paas,plates-formes de services>>
comme _Heroku_, _now_ et _Clever Cloud_.
Chaque projet d'application est accessible via un dépôt Git distant
(_remote_) : un dépôt est utilisé pour versionner notre code (GitHub par exemple)
tandis qu'un autre dépôt est utilisé pour réceptionner le code à déployer.

L'exemple suivant part du principe que notre terminal est positionné dans un
répertoire qui est un projet Git contenant au moins 1 commit.
Vous avez déjà configuré le dépôt distant à l'aide de
l'<<deploy.cli,outil de déploiement>> Heroku (cf. section précédente).

Nous pouvons vérifier si le dépôt est bien configuré à l'aide de la
commande `git remote` :

.Liste des dépôts distants d'un projet Git configuré pour Heroku.
----
$ git remote -v
<i>heroku</i>	https://git.heroku.com/mon-application.git (fetch)
<i>heroku</i>	https://git.heroku.com/mon-application.git (push)
origin	git@github.com:mon-compte/mon-application.git (fetch)
origin	git@github.com:mon-compte/mon-application.git (push)
----

Dans le cas d'Heroku, la commande `heroku git:remote` crée un _remote_ nommé
`heroku`.
Heroku redéploie notre application dès qu'on lui envoie du code en faisant
`git push heroku` :

----
$ git push heroku
> Counting objects: 4, done.
> Delta compression using up to 4 threads.
> Compressing objects: 100% (4/4), done.
> Writing objects: 100% (4/4), 17.77 KiB | 5.92 MiB/s, done.
> Total 4 (delta 2), reused 0 (delta 0)
> remote: Compressing source files... done.
> remote: Building source:
> remote:
> remote: -----> Node.js app detected
> remote:
> remote: -----> Creating runtime environment
> ...
> remote: -----> Launching...
> remote:        Released v30                     // <1>
> remote:        https://mon-application.herokuapp.com/ deployed
> remote:
> remote: Verifying deploy... done.
----
<1> C'est le trentième déploiement — on peut revenir à une version antérieure si nécessaire.

L'URL de l'application est rappelée dans les _logs_ du déploiement. +
En cas d'erreur, la version précédente de l'application reste en ligne.
Nous avons ainsi le temps de corriger le problème sans interruption de service.

[[deploy.clone]]
=== En faisant `git pull` lors d'une session SSH

La récupération du code source à distance avec Git et SSH est une manière de déployer
similaire à la mise à jour et au démarrage d'une application sur notre ordinateur.

Cette technique s'applique si notre application est placée sur un
<<hosting.shared,hébergement mutualisé>>,
<<hosting.vm,dédié ou virtualisé>> ou une <<hosting.cloud,offre _cloud_>>.

L'exemple suivant illustre l'initialisation d'un projet via la connexion
SSH à un <<hosting.shared,hébergement mutualisé>> chez alwaysdata.

.Première récupération d'un dépôt Git lors d'une session SSH.
----
$ ssh moncompte@ssh-moncompte.alwaysdata.net
$$ git clone https://github.com/moncompte/monprojet .
$$ npm install
----

Nous avons cloné un projet comme nous aurions pu le faire si on installait
notre projet depuis zéro sur notre ordinateur.

Dans le cas d'une mise à jour, nous récupérons les changements depuis le dépôt
distant en faisant `git pull`.
`npm install` mettra à jour les dépendances s'il y a des différences entre
le contenu du fichier `package.json` et les modules déjà installés
— voir le <<../chapter-05/index.adoc#,chapitre 5>> :

.Mise à jour d'une application lors d'une session SSH.
----
$ ssh moncompte@ssh-moncompte.alwaysdata.net
$$ git pull
$$ npm install
----

Dans le cas d'alwaysdata, l'application se redémarre depuis leur
<<hosting.shared,interface d'administration>>. +
Dans les autres cas, redémarrez l'application selon le procédé choisi après
avoir lu la section <<startup,démarrer automatiquement nos applications>>.


[[deploy.recipe]]
=== Avec une recette de déploiement (_Ansible_, _Chef_, etc.)

La recette de déploiement est la manière la plus complète de partager et
d'automatiser un déploiement complexe.

Cette méthode se place dans la continuité de
<<deploy.clone,`git pull` lors d'une session SSH>> : nous orchestrons les
actions nécessaires au déploiement en les listant dans un
fichier de configuration, en choisissant dans quel ordre les déclencher
et sur quel(s) serveur(s).

Nous retrouvons _Puppet_ ([URL]#https://puppet.com#),
_Chef_ ([URL]#https://www.chef.io#) et _Ansible_ ([URL]#https://ansible.com#)
parmi les outils les plus utilisés et les mieux documentés.
Ils ont chacun une philosophie de configuration et d'exécution différente
— l'idéal est encore d'essayer d'écrire une première recette avec chacun d'entre
eux pour voir celui qui vous semble le plus naturel à utiliser.

Ma préférence va vers _Ansible_ car le logiciel s'installe facilement
sur macOS et Linux, se configure avec une syntaxe que je connais déjà (_YAML_)
et je trouve ses messages d'erreurs informatifs.

L'exemple suivant illustre le déploiement de l'application Node _Slackin_
([URL]#https://github.com/rauchg/slackin#) sur
l'<<hosting.shared,hébergement mutualisé>> alwaysdata :

----
$ ansible-playbook  -i ansible/inventory.yaml ansible/playbook.yaml

PLAY [webservers] *******************************

TASK [Gathering Facts] **************************
ok: [ssh-moncompte.alwaysdata.net]

TASK [code source via git] **********************
ok: [ssh-moncompte.alwaysdata.net]

TASK [mise à jour des modules npm] **************
ok: [ssh-moncompte.alwaysdata.net]

PLAY RECAP **************************************
ssh-moncompte.alwaysdata.net : ok=3
----

La commande précédente a eu pour effet de créer des connexions SSH avec les
machines listées dans le fichier `inventory.yaml` puis de jouer les actions
listées dans le fichier `playbook.yaml`.

[horizontal]
.Concepts importants d'Ansible
Inventaire::
  *Liste de serveurs connus* sur lesquels effectuer des déploiements. +
  Les serveurs peuvent être catégorisés (par type, par emplacement)
  pour contrôler finement les actions à déclencher.
  Par exemple : uniquement les serveurs web de production,
  les bases de données de test, l'API de la région Europe.
Playbook::
  *Liste des actions possibles* en fonction des types de serveurs. +
  Ces actions peuvent être rejouées à l'infini et de manière prédictible.

Le _playbook_ suivant illustre 2 tâches appliquées uniquement sur
les serveurs étiquetés dans notre _inventaire_ en tant que `webservers` :

[source,yml]
.ansible/playbook.yaml
----
include::{sourceDir}/ansible/playbook.yaml[]
----
<1> Actions Git — pour en savoir plus [URL]#https://docs.ansible.com/ansible/2.5/modules/git_module#.
<2> Adresse du dépôt Git à récupérer.
<3> Indique de cloner le dépôt s'il n'est pas déjà présent.
<4> Indique de récupérer les commits du dépôt en faisant `git pull`.
<5> Actions npm — pour en savoir plus [URL]#https://docs.ansible.com/ansible/2.5/modules/npm_module#.
<6> Indique d'installer les dépendances npm en faisant `npm install`.
<7> Indique de lancer la mise en jour des modules npm avec l'option `--production` — c'est à dire sans les dépendances listées dans le champ `devDependencies`.

Les tâches sont réplicables sur les serveurs listés dans un fichier d'inventaire :

[source,yml]
.ansible/inventory.yaml
----
include::{sourceDir}/ansible/inventory.yaml[]
----

Nous déployons sur un seul serveur dans ce cas de figure.
Mais nous pourrions tout à fait déployer une même application avec la même
configuration sur une dizaine de serveurs (application à fort traffic)
ou une même application déployée chez plusieurs centaines de clients.
Dans tous les cas, l'application serait dans un état consistant sur toutes les
machines, avec peu de chances d'oublier une opération et une plus grande facilité
à revenir en arrière.

[[deploy.docker]]
=== En publiant une image Docker

Une image Docker est un moyen fiable de reproduire le même environnement
applicatif et ses dépendances sur plusieurs systèmes d'exploitation
— y compris Windows, Linux et macOS.

Un des objectifs de Node est de pouvoir faire fonctionner un script
sur tout système d'exploitation compatible.
Docker ([URL]#https://www.docker.com#) pousse cette compatibilité plus
loin en empaquetant tout ce qui est nécessaire au bon fonctionnement
de l'application (dépendances, logiciels système).
Le mécanisme d'exécution aide à la fois à orchestrer plusieurs conteneurs entre
eux — y compris bases de données et moteurs de recherche — et de pouvoir
revenir dans l'état applicatif initial.

Le fichier suivant est un exemple fonctionnel d'image Docker.
Son intention est de créer un environnement Node {nodeCurrentVersion}
pour une <<../chapter-07/index.adoc#,application web>> (cf. chapitre 7)
qui comporte une <<../chapter-05/index.adoc#,dépendance npm>> (cf. chapitre 5) :

[subs="+attributes"]
.Dockerfile
----
include::{sourceDir}/Dockerfile[]
----

Nous pouvons constater le choix de l'environnement Node (`FROM`),
avant de procéder à la copie des fichiers applicatifs vers l'image (`COPY`).
Suite à ça nous installons aussi les dépendances de l'application et spécifions
quelle commande effectuer lorsque l'image Docker est lancée (`CMD`).

L'image se construit et le conteneur se démarre sur notre ordinateur comme suit :

----
$ docker build -t nodebook/demo .
$ docker run -ti --rm -p 4000:4000 nodebook/demo
$ curl -L http://localhost:4000
----

Le transfert de l'image Docker vers un registre comme _Docker Hub_
([URL]#https://hub.docker.com#) garantit l'exécution de ce même environnement
applicatif, partout.

Nous avons déjà parlé de l'<<deploy.cli,outil en ligne de commande>>
du service _now_ ([URL]#https://zeit.co/now#) dans la section du même nom.
Il est aussi capable de déployer un conteneur Docker en se basant sur un fichier
`Dockerfile` en rajoutant l'option `--docker` :

----
$ now <i>--docker</i> --public
> Deploying ~/.../examples under oncletom
> https://examples-zlssezfiej.now.sh [in clipboard] (bru1) [7s]
> Synced 1 file (156B) [7s]
> Building…
> ▲ docker build
> Sending build context to Docker daemon 17.92 kBkB
> ▲ Storing image
> Build completed
> Verifying instantiation in bru1
> ✔ Scaled 1 instance in bru1 [18s]
> Success! Deployment ready
----

Une autre solution consiste à publier notre image sur _Docker Hub_,
la plate-forme officielle de partage d'images Docker.
Docker Hub dispose d'une fonctionnalité de construction automatique connectée
à GitHub.
Docker Hub construit l'image à chaque nouveau commit, puis la met à disposition.

.Création d'un _build_ automatisé à partir d'un dépôt GitHub.
image::images/docker-automated-build.png[width="85%"]

Il ne reste alors plus qu'à la collecter sur un ordinateur avec la commande
`docker pull` — que ce soit sur notre machine, chez notre hébergeur
ou par le biais du <<deploy.ci,service d'intégration continue>>.

[NOTE]
.[RemarquePreTitre]#Avancé# Amazon Elastic Container Registry
====
Le <<hosting.cloud,fournisseur _cloud_>> Amazon Web Services intègre
un registre privé d'images Docker pour chaque compte client.

_Elastic Container Registry_ (ECR, [URL]#https://aws.amazon.com/ecr/#)
se connecte à d'autres services comme _Amazon CodeDeploy_ pour déclencher
des mises à jour d'infrastructure à chaque nouvelle image Docker.
====


[[deploy.ci]]
=== En paramétrant un logiciel d'intégration continue

L'utilisation d'un logiciel d'intégration continue est la manière la plus
flexible d'automatiser tout type de déploiement.

L'intégration continue vise à vérifier si des régressions se sont glissées
dans notre code.
L'idée est de livrer régulièrement du code pour détecter les erreurs au plus tôt. +
Les services d'intégration continue automatisent cette pratique.
Ils s'intègrent avec d'autres services pour prévisualiser les branches,
compiler la documentation mais aussi pour déployer des artéfacts sur d'autres
plates-formes : <<../chapter-05/index.adoc#publish,registre npm>>,
GitHub Pages, Heroku ou même <<hosting.lambda,Amazon Lambda>>.

Le logiciel Jenkins ([URL]#https://jenkins.io/#) s'installe sur notre propre
infrastructure tandis que des services en ligne comme
Circle CI ([URL]#https://circleci.com#), Travis CI ([URL]#https://travis-ci.com#)
et CodeShip ([URL]#https://codeship.com#) mette à disposition leur
infrastructure gratuitement pour les projets _open source_.
GitLab ([URL]#https://www.gitlab.com#) combine l'hébergement de dépôts Git
et l'intégration continue.

[TIP]
.[RemarquePreTitre]#Windows# Service AppVeyor
====
J'utilise AppVeyor ([URL]#https://appveyor.com#) en complément d'un autre
service d'intégration continue quand il s'agit de tester
la *compatibilité du code avec Windows*
— ce qui est le cas des exemples de cet ouvrage.
====

J'ai une préférence pour GitLab lorsque le projet y est hébergé.
Sinon j'utilise pour Travis CI car j'aime la clarté de sa configuration
et de leur documentation.
J'ai aussi apprécié la qualité des échanges avec leur support technique
et leurs employés.

Le fichier suivant décrit un exemple fichier de configuration Travis CI.
Il se place à la racine d'un projet à tester et s'écrit avec la syntaxe _YAML_ :

[subs="+attributes"]
.{empty}.travis.yml
----
include::{sourceDir}/.travis.yml[]
----

Cet exemple est structuré en 3 parties :

. la *configuration de l'environnement* — en l'occurence Node {nodeCurrentVersion} ;
. la *commande de test* ;
. la *configuration du déploiement* en cas de succès.

Ici, le déploiement consiste à
<<../chapter-05/index.adoc#publish,déployer le code sur le registre npm>>
quand les tests passent lors de la création d'un tag Git.
Les variables d'environnement `$NPM_EMAIL` et `$NPM_TOKEN` se configurent de
manière sécurisée sur l'écran de configuration du projet
(voir illustration ci-après).

[NOTE]
.[RemarquePreTitre]#Documentation# .travis.yml
====
Une documentation adaptée aux projets Node est disponible à cette adresse :
[URL]#https://docs.travis-ci.com/user/languages/javascript-with-nodejs/#
====

.Écran de configuration des variables d'environnement sécurisées.
image::images/travisci-secrets.png[width="85%"]

L'exemple suivant illustre l'utilisation de
l'<<deploy.cli,outil en ligne de commande>> `now` dès qu'un nouveau commit
est poussé sur la branche `master` et que les tests passent au vert :

[subs="+attributes"]
.{empty}.travis.yml
----
include::{sourceDir}/.travis-now.yml[]
----

Les informations d'exécution des tests sont consignés au même titre que le statut
du déploiement — voir image ci-contre.

.Écran illustrant le déploiement automatique d'une application Node avec la commande `now`.
image::images/travisci-build.png[width="85%"]

[[hosting]]
== Choisir son hébergement

Nous allons nous intéresser aux différentes possibilités d'hébergement
d'applications Node.

Côté tarifs, certaines sont gratuites sous certaines conditions,
d'autres se paient à l'heure et d'autres à l'année.
Certaines offres sont figées, d'autres permettent de rajouter des machines
voire même de changer la puissance en cours de route.


[[hosting.paas]]
=== Plate-forme en tant que service (_Platform as a Service_, _PaaS_)

Les plates-formes de services *automatisent la configuration* et
*facilitent le déploiement* de nos applications Node mais également
Ruby, Python et PHP, entres autres.
Elles se spécialisent dans des déploiements rapides, une allocation des
ressources flexible, à la demande et en un clic.

C'est le *moyen le plus facile de déployer une application Node*, surtout si
on utilise déjà Git pour versionner son code.

Leur philosophie est de *tout penser en terme de ressources modulaires*.
On paie une certaine capacité de CPU et de RAM, à la minute ou à l'heure.
Ces capacités s'augmentent ou se réduisent à souhait, en quelques clics.
Le tout, sans changer une seule ligne de code dans notre application.

Le principe de déploiement sur les plates-formes de services est minimaliste :

. création d'une nouvelle application sur la plate-forme en ligne ;
. nous _poussons_ notre code applicatif — généralement avec `git` ;
. la plate-forme prépare la mise en ligne, installe les dépendances _npm_ et l'application ;
. notre application est fonctionnelle quelques secondes plus tard.

[format="csv", options="header"]
.Sélection de fournisseurs _PaaS_
|===
Service, Déploiement, Add-ons, Gratuité, Tarif
[URL]#https://zeit.co#, "cli", {cross}, 3 apps, 15$/mois/10 apps
[URL]#https://clever-cloud.com#, "git", {tick}, crédit 20€, 5€/mois/app
[URL]#https://gandi.net/hosting/simple#, "cli/git/SSH", {tick}, 10 jours, 5€/mois/app
[URL]#https://scalingo.com#, "git/GitHub", {tick}, 30 jours, 7€/mois/app
[URL]#https://heroku.com#, "cli/git/GitHub/Dropbox", {tick}, 1000 heures par mois, 7$/mois/app
|===

La startup californienne Zeit ([URL]#https://zeit.co#) édite le service de
_déploiements immutables_ nommé _now_ ([URL]#https://zeit.co/now#).
Ce service est focalisée sur l'hébergement de sites statiques,
d'applications web Node et de conteneurs Docker.

Sa particularité est de créer une *nouvelle instance d'application par déploiement*.
On ne modifie donc jamais un déploiement déjà existant.

C'est un service que j'apprécie pour sa simplicité.
C'est probablement le plus pratique à utiliser si vous n'utilisez pas Git
ou si vous n'avez pas de dépendance envers une base de données
comme MySQL, MariaDB ou Postgres.

_Heroku_ ([URL]#https://heroku.com#) est une autre alternative
pour démarrer en douceur sans sortir la carte bleue.
Des modules optionnels couvrent nos besoins en bases de données comme
_MySQL_, _MariaDB_, _redis_ ou _postgreSQL_ par exemple.
D'autres services complémentaires gèrent l'envoi d'emails,
l'indexation de contenus, le monitoring, les logs, etc.

.Ensemble de ressources complémentaires à une application Node hébergée sur Heroku.
image::images/heroku-addons.png[width="85%"]

S'il est facile de déployer sur ces infrastructures et de gérer les ressources
allouées à nos applications, à l'inverse la facture peut vite devenir salée
à mesure qu'on augmente leur puissance.
C'est tout relatif et rapport au coût de _notre temps_ passé à gérer les machines
si on devait tout faire à la main.


[[hosting.shared]]
=== Hébergement mutualisé

Les hébergements mutualisés ont l'avantage d'être bon marché
et ne demandent aucun entretien de notre part.
Cette formule est idéale pour faire ses premiers pas de mise en ligne
de sites web.

Leur modèle est adapté pour l'hébergement de fichiers statiques ou des sites web
construits avec des langages de scripts comme Python ou PHP.
Rares sont ceux qui ont adapté leur fonctionnement au modèle applicatif de Node.

Alwaysdata ([URL]#https://alwaysdata.com#) fait exception à la règle.
Ce service d'hébergement indépendant dispose d'une formule gratuite avec 100Mo
d'espace disque pour démarrer.

Le <<deploy,déploiement>> de nos applications se fait via
<<deploy.sftp,SSH ou SFTP>>, en <<deploy.clone,utilisant Git>>,
avec un <<deploy.ci,service d'intégration continue>> ou bien en
<<deploy.recipe,utilisant une recette>>.

L'interface d'administration référence une section *Sites* dans la barre de
navigation.
Cette section liste les différents sites de notre compte.
Si vous venez juste de créer le votre, un site a automatiquement été créé.
Son URL est déterminée à partir du *nom d'utilisateur* que vous avez choisi
lors de la phase d'inscription.

Un clic sur le bouton *Modifier* nous aidera à changer ses réglages :


.Liste de nos sites configurés chez alwaysdata.
image::images/alwaysdata-site-list.png[width="85%"]

Le nouvel écran mentionne les adresses auxquel le site répond.
En basculant vers un compte payant, on pourra assigner un ou plusieurs domaines
ou sous-domaines à ce même site.

.Écran de configuration d'un site chez alwaysdata.
image::images/alwaysdata-site-informations.png[width="85%"]

Les réglages liés à Node se trouvent sous les adresses.
Le _type_ de site doit être changé en `Node.js` pour afficher
les champs de configuration qui nous intéressent.

La _commande_ se configure de la même manière
que l'<<../chapter-04/index.adoc#script,exécution d'un script Node>>
— cf. <<../chapter-04/index.adoc#,chapitre 4>>.
On peut aussi faire appel au
<<../chapter-05/index.adoc#start,script `npm start`>> comme vu pendant
la lecture du <<../chapter-05/index.adoc#,chapitre 5>> :

.Écran de configuration de Node.js pour un site chez alwaysdata.
image::images/alwaysdata-site-configuration.png[width="85%"]

La commande complète devrait apparaître dans la section *Processus*
une fois la configuration sauvegardée.

.Liste des processus liés à nos sites chez alwaysdata.
image::images/alwaysdata-process-list.png[width="85%"]

En cas de doute, un bouton *Redémarrer* est affiché à côté du bouton *Modifier*
dans la liste des sites.
L'application sera alors interrompue et relancée.
Cette opération est nécessaire pour que l'application prenne en compte
les changements après une mise à jour ou un plantage.

[NOTE]
.[RemarquePreTitre]#Configuration# Une application Node par site
====
Alwaysdata nous permet d'associer un seul processus à un seul site.

Pour rendre une application Node accessibles sur Internet, il faudra
alors créer un nouveau site et lui associer un autre nom de domaine,
ou un sous-domaine.
====

[TIP]
.[RemarquePreTitre]#Aide# Forum d'entraide
====
L'équipe et la communauté alwaysdata ([URL]#https://forum.alwaysdata.com#) sont
sympathiques et à l'écoute.
C'est un endroit idéal pour chercher des informations et poser des questions
pour mieux comprendre ce empêche votre application de fonctionner sur leurs services.
====


[[hosting.cloud]]
=== Serveur virtualisé (VPS, VM), dédié ou cloud

La location d'un serveur dédié revient à payer pour un ordinateur complet,
son entretien physique et son placement dans un _datacenter_ — un immense
parking à ordinateurs connecté à un réseau haute-capacité.

Les _Virtual Private Servers_ (_VPS_) sont des machines virtuelles
(_Virtual Machine_, _VM_) : un serveur dédié dont les ressources sont réparties
en plusieurs unités indépendantes les unes des autres, les VM.

Les offres _cloud_ sont une version "élastique" des VM : la puissance de calcul,
la bande-passante et la mémoire allouées sont ajustables sans avoir à
changer de machine, sans avoir à tout réinstaller.
Ces ressources sont considérées comme étant "à la demande" : elles s'obtiennent
en quelques secondes et peuvent être mises en pause, réduites, augmentées ou
supprimées à tout moment.

Leur modèle de facturation s'adapte à la souplesse d'allocation des ressources :

- au mois : VPS, serveur virtualisé, serveur dédié
- à l'heure : serveur _cloud_
- à la (milli)seconde : <<hosting.lambda,fonction événementielle>>

[format="csv", options="header", separator=";"]
.Sélection de fournisseurs de serveur virtualisé et dédié
|===
Service; Déploiement; Add-ons; Tarif horaire; Tarif mensuel
[URL]#https://ovh.com/fr/vps/#; "SSH"; {tick}; -; 4.00€/VM
[URL]#https://online.net/fr/serveur-dedie#; "SSH, CLI, API"; {tick}; -; 14.50€/serveur
[URL]#https://alwaysdata.com/fr/pricing/#vps#; "SSH, API"; {tick}; -; 149.00€/VM
|===

[format="csv", options="header", separator=";"]
.Sélection de fournisseurs de serveur _cloud_
|===
Service; Déploiement; Add-ons; Tarif horaire; Tarif mensuel
[URL]#https://scaleway.com#; "CLI, SSH, API"; {cross}; 0.004€; 2.30€/VM
[URL]#https://linode.com#; "CLI, Git, API, Web"; {tick}; 0.0075$; 5.00$/VM
[URL]#https://gandi.net/hosting/iaas#; "CLI, Git"; {cross}; 0.0081€; 6.00€/VM
[URL]#https://ovh.com/fr/public-cloud/#; "SSH, API, Web"; {tick}; 0.062€; 26.00€/VM
[URL]#https://digitalocean.com#; "CLI, API"; {cross}; 0.007$; 5.00$/VM
[URL]#https://aws.amazon.com/fr/ec2/#; "CLI, API, SSH, Web"; {tick}; 0.0132$; 9.67$/VM
|===

[TIP]
.[RemarquePreTitre]#Avancé# HashiCorp Terraform
====
Le logiciel _Terraform_ ([URL]#https://terraform.io/#) a pour intention
de documenter une infrastructure (serveurs, DNS, stockage, etc.) sous forme
d'un fichier de configuration — versionnable avec Git.

C'est un outil idéal pour automatiser le déploiement d'une infrastructure de zéro
mais pour la faire évoluer d'une version à une autre.
Nous pouvons ainsi créer une architecture combinant plusieurs fournisseurs
sans gérer la complexité et la non-interopérabilité de leurs API.
====

[[hosting.lambda]]
=== Fonction événementielle (_Serverless_, _Lambda_)

La fonction événementielle est l'évolution ultime des offres _cloud_.
Au lieu de payer une machine ou une VM à l'heure,
*nous payons pour éxécuter du code à la milliseconde*.
Ce code se déclenche en réaction à événement se produisant ailleurs
sur l'infrastructure : une requête HTTP entrante, un nouveau fichier ou encore
un appel de l'API de l'hébergeur.

C'est le moyen le plus économique pour
*exécuter du code à tout instant sans payer le temps d'inactivité d'une machine*.
On pourrait comparer ce modèle à celui de la téléphonie mobile lorsqu'on a à choisir
entre un forfait (coût fixe même si on ne consomme pas tout) et un paiment à la carte
(coût dépendant de la consommation).

Les applications destinées à être exécutées comme fonction événementielle
ont une architecture un peu différente.
Au lieu de démarrer un serveur web basé sur le
<<../chapter-04/index.adoc#http,module `http`>>, nous exposons
une *fonction qui retourne un résultat de manière asynchrone* :

[source,javascript]
.webtask.js
----
include::{sourceDir}/webtask.js[]
----
<1> Le paramètre `context` contient des informations à propos de la requête entrante — paramètres, corps du message, etc.

Ce code est très similaire à ce que nous pourrions écrire lors de l'événement
`server.on('request')` du <<../chapter-04/index.adoc#http,module `http`>>.

Voyons ça en contexte dans l'interface web du service Webtask
([URL]#https://webtask.io/make#) :

.Exemple de fonction événementielle et de son historique d'exécutions avec le service Webtask.
image::images/webtask-make.png[width="85%"]

Un nom de Pokémon est affiché lorsque nous accédons à l'URL indiquée en bas
de l'écran.

[format="csv", options="header", separator=";"]
.Sélection de fournisseurs
|===
Service; Déploiement; Gratuité; Tarif des requêtes
[URL]#https://aws.amazon.com/lambda#; "Web, CLI, API"; 1M requêtes/mois; 0.2$/million
[URL]#https://webtask.io#; "Web, GitHub, CLI, API"; 1 requête/seconde; sur devis
[URL]#https://cloud.google.com/functions/#; "Web, GitHub, CLI, API"; 2M requêtes/mois; 0.4$/million
[URL]#https://zeit.co/now# + `micro`; "CLI, API"; 3 apps; 15$/mois
|===

Chaque fournisseur de fonction événementielle a sa propre vision des paramètres
qui nous sont donnés mais leur fonctionnement reste très proche.

Je trouve que Webtask est le service avec la plus faible courbe d'apprentissage.
Son interface y est pour beaucoup.

Le service _now_ est intéressant à plus d'un titre.
Il déploie avec un <<deploy.cli,outil en ligne de commande minimaliste>>,
y compris des <<deploy.docker,conteneurs Docker>>.
Il se transforme en fonction événementielle avec l'aide du
<<../chapter-05/index.adoc#modules,module npm>>
_micro_ ([URL]#https://npmjs.com/micro#).

Le service Amazon Lambda représente une marche d'apprentissage un peu plus importante.
C'est un service important de par l'outillage et la documentation disponibles
à son sujet.
Le service est complet, surtout une fois couplé avec le service
_Amazon API Gateway_.

[TIP]
.[RemarquePreTitre]#Avancé# _Amazon API Gateway_
====
Les Lambda d'Amazon ne sont pas accessibles depuis Internet par défaut.

Pour ce faire, il faut les relier au service  et associer
chaque route à une Lambda.
Le service se charge de transformer le résultat
— une chaîne de caractère, un tableau ou un objet ECMAScript —
en une réponse HTTP.
====


[NOTE]
.[RemarquePreTitre]#Définition# _Serverless_
====
Ce type d'infrastructure a été nommé _serverless_ suite à une organisation
du marché pour proposer des alternatives aux Lambda d'Amazon.

Quand on entend le mot _serverless_ — littéralement, sans serveur — il faut comprendre
"sans serveur à gérer soi-même".
L'hébergeur dispose quand même de machines pour exécuter le code.
Leurs ressources sont mutualisées au maximum.
====

== Améliorer la portabilité de notre application

Cette section va nous aider à préparer notre application Node pour l'exécuter
sur *d'autres environnements* que notre machine de développement sans changer
une seule ligne de code.

[[configuration]]
=== Configurer sans toucher à notre code

L'*environnement change entre notre machine de développement et la production*, le serveur de tests ou encore une plate-forme de services.
Le nom et paramètres d'accès à une base de données diffèrent, le système d'exploitation n'est pas le même ni même les codes d'accès pour accéder à certaines APIs.
Dans certains cas, nous n'avons même pas la main sur la décision d'un élément de configuration : il nous est fourni par l'environnement.
À nous donc de nous adapter.

Le premier élément que nous souhaitons maitriser est la *bonne version de Node* pour exécuter une application donnée.
Je recommande l'*utilisation d'un environnement par application* ou à défaut, d'une technologie de _conteneurs_ (comme LXC, Docker etc.) pour héberger plusieurs applications sur un même environnement.

Gérer plusieurs applications sur un même environnement implique soit de *_forcer_ toutes les applications* à se baser sur la même version de Node (mauvaise idée) soit à permettre à *chaque application d'expliciter sa préférence* (meilleure idée). +
Les _outils de gestion de version pour Node_ exposent justement un mécanisme pour indiquer la version désirée de *manière déclarative* via un fichier texte – ici, `.nvmrc` dans le cas de <<../chapter-02/index.adoc#nvm,nvm>> (cf. Chapitre 2).

[subs="+attributes"]
.{empty}.nvmrc
----
include::.nvmrc[]
----

Les commandes exposées par _nvm_ ont _connaissance_ de la version de Node déclarée dans le fichier `.nvmrc`.
Elles s'adapteront à cette version sauf à demander une version explicite :

[subs="+attributes"]
----
$ nvm install
$ nvm install {nodeNextVersion}
$ nvm run config/version.js
$ nvm run {nodeNextVersion} config/version.js
$ nvm exec npm start
----

Les cinq précédentes commandes permettent de :

. installer Node {nodeCurrentVersion} – version obtenue via `.npmrc` ;
. installer Node {nodeNextVersion} – version explicite ;
. exécuter le script `config/version.js` avec Node {nodeCurrentVersion} – version obtenue via `.npmrc` ;
. exécuter le script `config/version.js` avec Node {nodeNextVersion} – version explicite ;
. exécuter la commande `npm start` dans un environnement Node {nodeCurrentVersion}.

[CAUTION]
.[RemarquePreTitre]#Performance# Temps de démarrage
====
L'utilisation de _nvm_ entraine une pénalité d'environ _une seconde_ lors du _démarrage_ de l'application.
====

Nous n'avons certainement *pas envie de modifier le code* de notre application pour refléter ces différences.
C'est également l'*horreur de maintenir un fichier de configuration* par environnement.
Alors comment faire ?

L'utilisation de *variables d'environnement pour configurer une application* est la solution la plus aisée à implémenter.
Ces variables sont accessibles via l'objet `process.env` et *ne sont pas changer pendant la durée d'exécution d'une application*.
Voici un échantillon de leurs usages :

* *environnement d'exécution* (voir l'encadré sur `NODE_ENV` ci-après) ;
* *adresses de connexion aux bases de données* (_Data Source Name_, _DSN_) ;
* *URL des ressources et APIs* dont l'application dépend ;
* *autres variables* décrivant la cible de déploiement.

[TIP]
.[RemarquePreTitre]#À savoir# `NODE_ENV`
====
La variable d'environnement `NODE_ENV` a été adoptée par de nombreux outils
et _frameworks_ pour déterminer le contexte d'exécution d'une application,
effectuer des optimisations ou afficher des informations de débug supplémentaires.

Je *recommande d'expliciter `NODE_ENV=production`* dans un environnement de production.

* `development` +
  C'est le contexte assumé par défaut lorsqu'une personne développe sur sa machine.
  Les exceptions sont affichées de manière verbeuse et du code supplémentaire .
  Des informations sensibles peuvent être contenues dans les traces d'erreurs ;
* `test` +
  C'est la valeur que l'on choisit pour exécuter des tests applicatifs,
  augmenter la verbosité des _logs_ et activer les _Source Maps_ pour faciliter le débogage de fichiers transpilés.
  Certains _frameworks_ de test et outils d'_intégration continue_ assignent
  cette variable d'environnement par défaut ;
* `production` +
  Les exceptions n'affichent pas de détails – il faut aller
  <<exceptions,inspecter les erreurs et exceptions>>, l'outillage
  d'introspection est désactivé et la verbosité des _logs_ est supposée être
  moins verbeuse.
  On constate généralement des *améliorations de performance*.
====

L'exemple suivant décrit comment définir le _port réseau_ sur lequel écoutera le serveur HTTP.
Le numéro du port sera déterminé soit par le contexte d'exécution (_environnement de test_ ou _environnement de production_) soit par un numéro de port explicitement passée en tant que _variable d'environnement_.

[source%interactive,javascript,nodeVersion={nodeCurrentVersion}]
.config/env.js
----
include::{sourceDir}/config/env.js[]
----
<1> Décompose `PORT` de `process.env` et si la clé n'existe pas, assigne la valeur de `defaultPort`.

Jouons avec le script `config/env.js` pour illustrer ces différents scénarios :

----
$ PORT=8000 node config/env.js     # <1>
$ NODE_ENV=test node config/env.js # <2>
$ node config/env.js               # <3>
----
<1> Affiche `En écoute sur http://localhost:8000`.
<2> Affiche `En écoute sur http://localhost:3001`.
<3> Affiche `En écoute sur http://localhost:3000`.

Le mécanisme des _variables d'environnement_ est détaillé dans la section <<../chapter-04/index.adoc#process-env,variables d'environnement>> du chapitre 4.

L'utilisation de fichiers de configuration peut se révéler être un bon _complément_ aux variables d'environnement.
Ces fichiers de configuration doivent *s'appliquer à tous les contextes d'exécution* (production comme développement) pour être au plus proche de l'environnement de production à tout moment mais aussi pour simplifier la maintenance.

Autrement dit *oui* à un fichier configurant des outils de la même manière pour tout le monde (eslint, babel etc.) mais *non* à un fichier _production.json_, _dev.json_ etc.

Le fichier `package.json` est adapté au stockage d'informations de configuration.
Il est idéal pour y stocker différentes _valeurs par défaut_ comme le montre l'exemple suivant avec la clé `config.port` :

[source,javascript]
.package.json#L6-12
----
include::package.json[lines=6..12, indent=0]
----

L'appel du fichier suivant affichera un résultat différent selon que l'invocation se fasse avec Node ou _npm_ (cf. <<../chapter-04/index.adoc#invoke,invoquer Node.js>>, Chapitre 4) :

.Exécution de `config/file-npm.js` avec Node puis _npm_.
----
$ node config/file-npm.js
$ npm run config:file-npm
----

[source,javascript]
.config/file-npm.js
----
include::{sourceDir}/config/file-npm.js[]
----
<1> Affiche `undefined` avec Node et `3000` avec _npm_.
<2> Affiche `3000` avec Node et _npm_.

_npm_ aplatit la structure d'objet du fichier `package.json`, séparera chaque niveau de profondeur par le caractère `\_`, le préfixera par `npm_package_` et l'injectera dans l'objet `process.env`.

[TIP]
.[RemarquePreTitre]#Astuce# Combinaison avec les variables d'environnement
====
Un bon moyen de ne pas inscrire en dur l'emplacement d'un fichier de configuration est encore d'indiquer son emplacement via une variable d'environnement :

----
CONFIG_FILE=~/.secured/config.json node app.js
----
====

[[data-persistence]]
=== Persister les fichiers en dehors de notre application

Développer une application sur sa machine et la tester nous met à l'abri de certaines questions et considérations. +
Que se passe-t-il en cas de redémarrage de notre application ?
Et du système d'exploitation ?
Et si l'on doit installer l'application sur une nouvelle machine ?

On voudra s'assurer que nous ne perdrons pas de données lors d'une mise en production ou d'une mise à jour en production.
Autrement dit, il nous faut nous assurer que le bon support de persistence a été choisi pour les différents cas d'usage des données gérées.
Par exemple :

* une *liste d'utilisateurs inscrits* sur un service – on ne peut pas la perdre ;
* une *liste de sessions* – c'est acceptable de les remettre à zéro ;
* des *paramètres en cache* – on peut les reconstruire à la volée ;
* des *tâches de fond* – on veut les dissocier du processus applicatif.

*Arrêter et démarrer une application ne devrait pas entrainer de perte de données*.
Démarrer une application et l'accès à la source d'informations devrait *toujours fonctionner de la même manière* et ce, que ce soit son premier lancement, suite à une mise à jour ou après avoir déplacé l'application sur un différent support physique.

Enfin, il faut se poser la question de la _multiplicité_ : lancer plusieurs fois la même application Node—sur la même machine ou sur une machine différente—impacte-t-il la manière dont l'information est stockée et accédée ?
Cela entraine-t-il doublon ou corruption de données dans les valeurs enregistrées ?

Toutes ces questions devraient nous aiguiller vers le *choix d'un ou de plusieurs supports de persistence de données*.
Nous répondons au <<../chapter-07/index.adoc#database,choix d'une base de données>> dans le chapitre 7.
Nous verrons plus loin dans ce chapitre comment <<database-migration,effectuer des migrations de base de données>> afin de répercuter des évolutions dans leur structure et ce, sans intervention manuelle.



=== Exécuter l'application avec un utilisateur sans privilèges

Retenez une chose : *ne démarrez _jamais_ une application Node sur le port 80 ou 443* — ports utilisés par convention pour parler respectivement les protocoles HTTP et HTTPS.

L'exemple suivant illustre un serveur web Node qui écoutera sur le port 80 :

[source,javascript]
.server-port80.js
----
include::{sourceDir}/server-port80.js[]
----

Cela ne fonctionnera pas, y compris si le port est libre.
Tous les ports inférieurs à 1024 nécessitent que le programme soit lancé avec un utilisateur privilégié, comme _root_ par exemple.

Alors pourquoi ne pas utiliser _root_ pour avoir accès au port 80 ou 443 ?
<<system-security,Pour des raisons de sécurité>> comme évoqué précédemment dans ce chapitre.
Donc même si c'est techniquement faisable, *non nous le ferons pas* et nous privilégierons d'autres mécanismes, notamment celui du <<reverse-proxy,_reverse proxy_>>.


[TIP]
.[RemarquePreTitre]#Sécurité# Utilisateur sans privilèges
====
Un utilisateur sans privilège nommé `nobody` existe sur la plupart des systèmes d'exploitation.
Nous pouvons l'utiliser pour exécuter un processus critique. +
Cet utilisateur aura tout de même accès en lecture et en écriture aux fichiers et répertoires ouverts à tous les utilisateurs du système—l'impact devrait être limité.

----
$ sudo -u nobody node debug.js
$ ps aux | grep nobody
----

L'exécution de ces deux commandes lance un des exemples de ce chapitre en tant que `nobody`.
La liste des processus montre ensuite que le script `debug.js` est bien pris en charge par l'utilisateur `nobody`.
====

[TIP]
.[RemarquePreTitre]#Alternative# authbind
====
Le logiciel libre pour Linux _authbind_ comble le besoin impérieux d'assigner un port privilégié mais à un _utilisateur sans privilège_.

Il conviendra quand même de créer un utilisateur système avec des permissions limitées spécifiquement pour chaque application Node.

- [URL]#http://manpages.ubuntu.com/authbind#
====

[[database-migration]]
=== Scripter les changements de schémas de base de données

TBD.

=== Utiliser Docker pour tester un environnement reproductible

TBD.

[[startup]]
== Démarrer automatiquement nos applications

Qu'est-ce qui différencie le fonctionnement d'une application Node sur un _ordinateur de développement_ d'un _serveur de production_ ?
La manière de démarrer cette application.

Le démarrage d'une application Node sur notre machine de développement est une chose intuitive car manuelle.
Nous *démarrons le processus à la main*.
Nous avons d'ailleurs abordé différentes manières d'<<../chapter-04/index.adoc#invoke,invoquer un processus Node>> dans le <<../chapter-04/index.adoc#,chapitre 4>>.

Lorsqu'il s'agit d'un environnement de production, nous avons à _automatiser_ cette opération manuelle.

Nous allons voir différentes méthodes aboutissant à un résultat similaire.

=== Déléguer au service d'hébergement

Certains services d'hébergement prennent en charge le démarrage automatique de nos applications.
C'est le cas notamment des hébergements de type <<paas,_Platform as a Service_>> (_PaaS_) que nous explorerons plus en détails dans une section ci-après.

Ces services vont se baser sur des *conventions* ou des fichiers de *configuration* pour comprendre la nature de notre application et l'intégrer automatiquement.
Ils se baseront souvent sur le <<../chapter-05/index.adoc#npm-scripts,script `npm start`>> et d'autres informations contenues dans le fichier `package.json`.


[[process-manager]]
=== Avec un gestionnaire de processus

Les _gestionnaires de processus_ s'intègrent entre le système d'exploitation et nos applications.
Ils servent à *gérer leur démarrage* que ce soit en développement ou en production.
Ils déclarent nos applications en tant que *services système de manière universelle*, peu importe le système d'exploitation.

Autrement dit, un gestionnaire de processus prend en charge le *démarrage d'un processus* en tâche de fond, le *suivi de leur statut* ainsi que l'<<system-service,intégration en tant que *service système*>>.
Certains gestionnaires vont jusqu'à l'*inspection des logs* et le *passage à l'échelle* afin notamment de répartir les processus applicatifs sur chaque CPU de la machine.

*pm2* ([URL]#http://pm2.io/#) est un gestionnaire de processus populaire dans la communauté Node.
Il est lui-même écrit en ECMAScript et fonctionne aussi bien sous Linux, Windows que macOS.

La commande suivante démarrera une application web en lui attribuant un nom (ici, _nodebook-ch06-testapp_).
L'application sera démarrée en mode _cluster_ à raison d'un processus par CPU afin de répartir le trafic en fonction de la charge.

----
$ pm2 start app.js -i 0 --name nodebook-ch06-testapp
----

.Liste des processus gérés par _pm2_.
image::images/pm2-status.png[width="85%"]

L'ordinateur servant à la rédaction de l'ouvrage dispose de 4 CPU donc _pm2_ crée autant de processus en parallèle.

L'application de démonstration affiche un nom aléatoire de Pokémon ainsi que l'identifiant du processus au sein du _cluster_ lors de la consultation de la page [URL]#http://localhost:4000#. +
On s'en rend compte notamment en utilisant le programme `curl` (ou équivalent) :

----
$ curl --location http://localhost:4000
> [cluster #2] Jynx

$ curl --location http://localhost:4000
> [cluster #1] Starmie
----

Les processus peuvent être stoppés indépendamment.
L'application peut être redémarrée automatiquement en cas de plantage, et sans interruption après une mise à jour de la base de code, entre autres.

[TIP]
.[RemarquePreTitre]#Documentation# pm2
====
La documentation du projet _pm2_ liste en détails les actions et modules du programme.
Des tutoriaux sont également à disposition pour décrire pas à pas différentes intégration, notamment avec le serveur HTTP _nginx_ ou d'autres langages comme _TypeScript_.

- `$ pm2 --help`
- [URL]#http://pm2.keymetrics.io/#
- [URL]#https://github.com/Unitech/pm2#
====

[TIP]
.[RemarquePreTitre]#Alternative# foreman
====
_foreman_ est un gestionnaire de processus très simple à appréhender.
Il est utilisé pour déclarer et démarrer des applications sur la plate-forme d'<<paas,hébergement Heroku>>.

- [URL]#https://rubygems.org/gems/foreman#
- [URL]#http://ddollar.github.io/foreman/#
====


[[system-service]]
=== En créant un service système

On l'oublierait presque mais il n'y a pas besoin d'installer de logiciel pour démarrer une application automatiquement.
Après tout, c'est une des *responsabilités du système d'exploitation*.

Chaque système d'exploitation se base sur un *gestionnaire de processus*.
Nous pouvons lui communiquer les informations relatives au lancement de notre programme :

* *quelle(s) commande(s) exécuter* ;
* dans *quel ordre*, à *quel moment* et *après* quel(s) service(s) ;
* la *stratégie de reprise* (laisser le processus hors-service, redémarrer automatiquement etc.) ;
* *quelles opérations* effectuer avant et/ou après son démarrage, arrêt, redémarrage.

Nous déclarons ces informations dans un fichier de configuration, sous forme textuelle, souvent à raison d'un fichier de configuration par programme.
La syntaxe varie selon les gestionnaires de processus mais la philosophie reste identique. +
*systemd* est l'outil de gestion de processus de la distribution link:http://www.ubuntu-fr.org/[Linux Ubuntu] mais aussi d'autres distributions populaires comme _Debian_, _Fedora_ et _CentOS_.

[source]
.systemd/nodebook.d/app.conf
----
include::{sourceDir}/systemd/nodebook.d/app.conf[]
----

L'exemple précédent décrit l'initialisation notre service après le démarrage du service en charge des interfaces réseau (`NetworkManager.service`).
_systemd_ exécute la commande `npm start` en tant qu'utilisateur `nobody` (sans privilèges donc) dans le répertoire précisé par la directive `WorkingDirectory` et injecte la variable d'environnement `NODE_ENV`.
Enfin, _systemd_ redémarrera le service en cas d'erreur, dans une limite de 5 tentatives en l'espace de 120 secondes.

Si le fichier de configuration est accessible en tant que `/etc/systemd/nodebook.d/app.conf`, le service `nodebook` peut être démarré manuellement comme suit :

----
$ sudo systemctl start nodebook.service
----

D'autres commandes comme `reload`, `stop` et `restart` nous donnent le contrôle sur le cycle de vie de nos services. +
Dans tous les cas, le service sera démarré automatiquement au prochain démarrage du système d'exploitation.

[TIP]
.[RemarquePreTitre]#Documentation# systemd
====
Une introduction ainsi qu'une documentation détaillée de _systemd_ sont disponibles en ligne :

- [URL]#https://doc.ubuntu-fr.org/systemd# (en français)
- [URL]#https://freedesktop.org/software/systemd/man/# (en anglais)
====

[TIP]
.[RemarquePreTitre]#Alternative# Et pour Windows ?
====
La création d'un service système sur un serveur Windows implique une toute autre procédure.
Le service peut être manuellement créé depuis le *menu Services* ou scripté via l'intermédiaire du module npm `node-windows`.

- [URL]#https://npmjs.com/node-windows#
====


[[application-manager]]
=== Avec un serveur d'applications web

Un serveur d'application web a deux objectifs majeurs : *gérer le trafic HTTP* de manière optimale et *gérer le cycle de vie* d'une application web lors d'incidents ou de mises à jour par exemple.
Ce type de logiciel se _place entre_ une application Web et le monde extérieur.
Il peut être installé soit de manière *autonome* soit en tant que *module d'un serveur HTTP* tiers.

Leur *excellente gestion d'HTTP* permet de nous reposer sur les serveurs d'applications web pour gérer la charge des requêtes, de se protéger contre servir et mettre en cache les fichiers statiques, rester disponible même si l'application Node est tombée, entre autres.

Leur *intégration avec des plates-formes comme Node* alloue aux serveurs d'applications web la capacité de redémarrer une application Node si elle a planté ou a été mise à jour et ce, sans discontinuité de service (_zero downtime_).
Certains serveurs ont également la faculté de passer à l'échelle en *attribuant les applications Node aux CPU disponibles* afin de répartir la charge en fonction des ressources disponibles.

*Phusion Passenger* ([URL]#https://phusionpassenger.com/#) est un serveur d'applications web open source et compatible avec des applications Ruby, Node et Python.
Il s'installe de manière autonome ou en module pour _nginx_ ([URL]#http://nginx.org/#) et _Apache httpd_ ([URL]#https://httpd.apache.org/#).
Regardons ensemble l'intégration de _Passenger_ au travers de la configuration de son module _nginx_ :

[source]
.phusion-passenger/sites-available/webapp-nodebook.conf
----
include::{sourceDir}/phusion-passenger/sites-available/webapp-nodebook.conf[lines=2..-1]
----

L'exemple de configuration précédent déclare une application Node `app.js` placée dans le répertoire `/var/www/webapp-nodebook` et dont les fichiers statiques (JavaScript, CSS, images etc.) sont placés dans le sous-répertoire `public/`.

_Passenger_ démarrera au minimum 1 processus du fichier `app.js` dans une limite de 10, en fonction du traffic HTTP.
Chacun des processus sera démarré sous l'_utilisateur_ et le _groupe Unix_ propriétaire du fichier `app.js`.

_Passenger_ peut être contrôlé par l'intermédiaire du programme `passenger-config`, notamment avec la sous-commande `restart-app` :

----
$ passenger-config restart-app /var/www/webapp-nodebook
----

[TIP]
.[RemarquePreTitre]#Documentation# Phusion Passenger
====
Un guide pas à pas de configuration d'une application Node avec Passenger est disponible sur leur site officiel.

- [URL]#https://phusionpassenger.com/library/walkthroughs/deploy/nodejs/#
- [URL]#https://phusionpassenger.com/library/config/nginx/reference/#
====

Je recommande fortement cette approche, relativement facile à installer et à configurer, notamment de manière automatisée avec un serveur d'intégration continue.

Nous trouverons des informations sur le fonctionnement de Node derrière un <<reverse-proxy,_reverse proxy_>> dans une autre section de ce chapitre.


[[monitoring]]
== Points de vigilance après une mise en ligne

Cette section s'intéresse à la *détection proactive des incidents* et au
*suivi de la santé de notre application* une fois mise en ligne.
Nous allons ainsi apprendre à *dresser un diagnostic sans nous connecter* sur
la ou les machines hébergeant l'application en partie car nous n'y avons pas
toujours accès.


[[uptime]]
=== Être prévenu·e quand l'application plante

TBD.

////
https://www.pingdom.com/free
https://uptimerobot.com/
////


.Message d'erreur du serveur _nginx_ lorsque l'application Node ne répond plus.
image::images/nginx-proxy-error.png[width="85%"]

On cherchera entres autres à tester la réaction de notre code face à
des erreurs provoquées volontairement.

* *ressources indisponibles* +
  Il arrive aussi que notre code ne soit pas incriminé mais que la base de
  données devienne inaccessible pendant l'exécution de notre application.
  Ou qu'une clé d'API soit révoquée.
  Ou encore qu'une API soit indisponible, peu importe la raison—panne réseau,
  attaque destinée à faire tomber le système etc. +
  Le mieux que l'on puisse faire est d'utiliser des
  *modules _npm_ favorisant la reconnexion* aux bases de données,
  à *gérer l'indisponibilité de la ressource* (en ayant un modèle de réponse
  dégradé) ou encore en utilisant la technique du _backoff_ exponentiel pour
  retenter un certain nombre de fois en augmentant le temps entre temps chaque essai.
* *effets de bord système* +
  Le système d'exploitation peut nous jouer des tours sans que notre
  application ne soit réellement au courant.
  C'est le cas lorsque la _charge système_ dépasse la capacité de traitement
  des processeurs : notre application mettra plus de temps à exécuter toutes
  les opérations.
  Cela se produira également si l'espace disque tend à devenir faible : le
  système va _swapper_ et ralentira l'écriture de fichiers, notamment
  les _logs_ générés par le trafic de notre application. +
  La mise en place en place de <<monitoring,_monitoring_>> nous donnera
  une vue d'ensemble des ressources disponibles de notre parc machine.


[[exceptions]]
=== Consulter les erreurs applicatives

TBD.

////
https://sentry.io + https://docs.sentry.io/clients/node/
https://newrelic.com/nodejs
////

[[security.node]]
=== Mettre à jour Node en cas de faille de sécurité

////
https://nodejs.org/en/security/
////


[[security.npm]]
=== Mettre à jour les modules npm en cas de faille de sécurité

Il existe quatre niveaux où des failles de sécurité peuvent s'immiscer :

* *notre propre code* +
  Des revues de code, une amélioration de vos connaissances et la commande d'audits nous aideront à identifier les possibles failles et vulnérabilités ;
* *nos dépendances* +
  Mais aussi dans les dépendances de nos dépendances ;
* *Node* +
  Le code de certains modules Node – ou leur intégration avec un système d'exploitation spécifique – contient des failles ou fuites mémoire qui sont corrigées au fil des versions ;
* *dépendances de Node*
  Il est arrivé à plusieurs reprises que des vulnérabilités soient décelées dans _libuv_, _OpenSSL_ ou _V8_ — seule une mise à jour de Node peut mettre à ces dépendances.

On n'oubliera pas les vulnérabilités liés à notre système d'exploitation et celui de nos serveurs de production ainsi qu'à leur exposition au monde extérieur.

Plusieurs canaux sont à notre disposition pour être notifié très rapidement de mises à jour correctives :

* *Failles de sécurité de modules npm* : [URL]#https://nodesecurity.io/advisories# — et son flux RSS [URL]#https://nodesecurity.io/rss.xml# ;
* *Failles de sécurité de Node* : son flux RSS [URL]#https://nodejs.org/en/feed/vulnerability.xml# ;
* *Mises à jour de Node* : [URL]#https://nodejs.org/en/blog/# — et son flux RSS [URL]#https://nodejs.org/en/feed/blog.xml#.

////
- https://snyk.io/
- https://nodesecurity.io/
- greenkeeper
////


== Conclusion

TBD.
